{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Notes on Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I won't discuss the architecture of neural networks yet or why neural networks work. Instead, I'll cover some of the basic equations and some of the intutition/proofs behind them. For a more full (but introductory) discussion on neural networks, see [this](http://neuralnetworksanddeeplearning.com/chap1.html)*.\n",
    "\n",
    "A neural network, at the end of the day, takes an input, a vector $x$, and produces an output, a vector $a$. In order for a neural network to learn, it must (1) have a good architecture and (2) learn to produce a correct output given an input vector $x$. \n",
    "\n",
    "I won't discuss (1) completely, because (2) is the more mathematically involved part that I personally want to cover. Before discussing (2) however, I want to discuss neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neurons</h2>\n",
    "\n",
    "Neurons are nodes that accept an input and produce an output, albeit on a smaller scale than a neural network. A node will take an input $x$ (which might not be the same as the input vector given to the neural network initially depending on what layer the neuron is in) and output $\\sigma(wx + b)$ where $w$ and $b$ are weight and bias vectors, respectively. In component form, $\\sigma(wx + b) = \\sigma(\\sum_j w_jx_j + b)$. $\\sigma$ is an *activation function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient Descent</h2>\n",
    "\n",
    "What we would like to do is have the output vector of the neural network $a$ be similar to the desired output $y(x)$ where $x$ is the input vector (it's ok if we don't get an exact match, we just need it to be \"close enough\"). Since $a$ and $y(x)$ are vectors, we can define a *cost function* to minimize the difference between them, i.e, try to get these vectors to differ by a small amount. There are many types of cost functions, but a simple one is the mean-squared error (MSE) cost function:\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n}\\sum_x \\parallel y(x) - a\\parallel^2$\n",
    "\n",
    "Here, $w$ and $b$ are the weights and biases in the entire neural network, and $n$ is the total number of training inputs (this is so that we can minimize the difference between $y(x)$ and $a$ for all training inputs $x$). Clearly, $C$ is a function of $w$ and $b$ since $a$ is computed as a function of $w$ and $b$. Now, we *could* try and write out $C(w, b)$ explicitly as a function of all the weights and biases in the network, but that would be very difficult and is heavily dependent on the architecture of the neural network in question. Instead, we can try to decrease the cost by some $\\Delta C$ which, again, is a function of all the weights and biases in the network. However, if it is a function of all weights and biases in the network, then by minimizing the weights and biases, we can minimize $C$. Using the gradient descent method:\n",
    "\n",
    "$w_k \\rightarrow w_k\\prime = w_k - \\eta\\frac{\\partial C}{\\partial w_k}$\n",
    "\n",
    "$b_l \\rightarrow b_l\\prime = b_l - \\eta\\frac{\\partial C}{\\partial b_l}$\n",
    "\n",
    "We can modify this to become stochastic gradient descent by summing over the partial derivatives of $C$ with respect to $w_k$ or $b_l$ in the current mini-batch (essentially, we estimate the true gradient by looking at a small sample of training inputs). Once we've exhausted all mini-batches, we would have completed an epoch of training.\n",
    "\n",
    "$w_k \\rightarrow w_k\\prime = w_k - \\frac{\\eta}{m}\\sum_j\\frac{\\partial C_{X_j}}{\\partial w_k}$\n",
    "\n",
    "$b_l \\rightarrow b_l\\prime = b_l - \\frac{\\eta}{m}\\sum_j\\frac{\\partial C_{X_j}}{\\partial b_l}$\n",
    "\n",
    "So we have a way to minimize the weights and biases of the neural network. However, the reason we use gradient descent in the first place was because using calculus to find the global minimum of $C(w, b)$ is complicated and because it's not computationally efficient to compute derivatives. However, even if we use stochastic gradient descent, it still seems we have to compute the partial derivatives $\\frac{\\partial C_{X_j}}{\\partial w_k}$ and $\\frac{\\partial C_{X_j}}{\\partial b_l}$. In the next section, we discuss backpropagation, which provides us a method to compute these partial derivatives for a given training input for the whole network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
